{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "airplane\t\t\t\t\t\t\t\t\t\t\n",
    "automobile\t\t\t\t\t\t\t\t\t\t\n",
    "bird\t\t\t\t\t\t\t\t\t\t\n",
    "cat\t\t\t\t\t\t\t\t\t\t\n",
    "deer\t\t\t\t\t\t\t\t\t\t\n",
    "dog\t\t\t\t\t\t\t\t\t\t\n",
    "frog\t\t\t\t\t\t\t\t\t\t\n",
    "horse\t\t\t\t\t\t\t\t\t\t\n",
    "ship\t\t\t\t\t\t\t\t\t\t\n",
    "truck\t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File path as a string\n",
    "\n",
    "CIFAR_DIR = 'cifar-10-batches-py/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will describe the layout of the Python version of the dataset. The layout of the Matlab version is identical. \n",
    "\n",
    "The archive contains the files data_batch_1, data_batch_2, ..., data_batch_5, as well as test_batch. Each of these files is a Python \"pickled\" object produced with cPickle. Here is a python3 routine which will open such a file and return a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
    "label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs = ['batches.meta','data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5','test_batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = [0, 1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, direc in zip(all_data, dirs):\n",
    "    all_data[i] = unpickle(CIFAR_DIR + direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_meta = all_data[0]\n",
    "data_batch1 = all_data[1]\n",
    "data_batch2 = all_data[2]\n",
    "data_batch3 = all_data[3]\n",
    "data_batch4 = all_data[4]\n",
    "data_batch5 = all_data[5]\n",
    "test_batch = all_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_cases_per_batch': 10000,\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the label names and 'b' stands for bytes literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "* data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "* labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
    "\n",
    "* label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To display a single image using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_img = data_batch1[b\"data\"]\n",
    "single_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To reshape image for viewing\n",
    "\n",
    "single_img = single_img.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16ef26c3fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16ef2575908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(single_img[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions to deal with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To one-hot encode the 10 possible labels\n",
    "\n",
    "def one_hot_encode(vec, vals = 10):\n",
    "    \n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CifarHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        \n",
    "        # Grabs a list of all the data batches for training\n",
    "        self.all_train_batches = [data_batch1,data_batch2,data_batch3,data_batch4,data_batch5]\n",
    "        # Grabs a list of all the test batches (really just one batch)\n",
    "        self.test_batch = [test_batch]\n",
    "        \n",
    "        # Intialize some empty variables for later on\n",
    "        self.training_images = None\n",
    "        self.training_labels = None\n",
    "        \n",
    "        self.test_images = None\n",
    "        self.test_labels = None\n",
    "    \n",
    "    def set_up_images(self):\n",
    "        \n",
    "        print(\"Setting Up Training Images and Labels\")\n",
    "        \n",
    "        # Vertically stacks the training images\n",
    "        self.training_images = np.vstack([d[b\"data\"] for d in self.all_train_batches])\n",
    "        train_len = len(self.training_images)\n",
    "        \n",
    "        # Reshapes and normalizes training images\n",
    "        self.training_images = self.training_images.reshape(train_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        # One hot Encodes the training labels (e.g. [0,0,0,1,0,0,0,0,0,0])\n",
    "        self.training_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.all_train_batches]), 10)\n",
    "        \n",
    "        print(\"Setting Up Test Images and Labels\")\n",
    "        \n",
    "        # Vertically stacks the test images\n",
    "        self.test_images = np.vstack([d[b\"data\"] for d in self.test_batch])\n",
    "        test_len = len(self.test_images)\n",
    "        \n",
    "        # Reshapes and normalizes test images\n",
    "        self.test_images = self.test_images.reshape(test_len,3,32,32).transpose(0,2,3,1)/255\n",
    "        # One hot Encodes the test labels (e.g. [0,0,0,1,0,0,0,0,0,0])\n",
    "        self.test_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.test_batch]), 10)\n",
    "\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        # Note that the 100 dimension in the reshape call is set by an assumed batch size of 100\n",
    "        x = self.training_images[self.i:self.i+batch_size].reshape(100,32,32,3)\n",
    "        y = self.training_labels[self.i:self.i+batch_size]\n",
    "        self.i = (self.i + batch_size) % len(self.training_images)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Training Images and Labels\n",
      "Setting Up Test Images and Labels\n"
     ]
    }
   ],
   "source": [
    "# Before Your tf.Session run these two lines\n",
    "ch = CifarHelper()\n",
    "ch.set_up_images()\n",
    "\n",
    "# During your session to grab the next batch use this line\n",
    "# (Just like we did for mnist.train.next_batch)\n",
    "batch = ch.next_batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two placeholders for x and y_true\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])               # 32 x 32 pixels and 3 color channels\n",
    "y_true = tf.placeholder(tf.float32, shape = [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another placeholder for dropout\n",
    "\n",
    "hold_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the helper functions for the weights, biases, conv2d, maxpool2x2, convLayer, fullLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_distrib = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(init_random_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(init_bias_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the conv2d function, we'll return an actual convolutional layer here that uses an ReLu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fully Connected layer:\n",
    "\n",
    "def full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating the layers:\n",
    "\n",
    "conv_1 = conv_layer(x, shape = [4, 4, 3, 32])\n",
    "conv_1_pool = maxpool2x2(conv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_2 = conv_layer(conv_1_pool, shape = [4, 4, 32, 64])\n",
    "conv_2_pool = maxpool2x2(conv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Flattening the layer by reshaping the pooling layer into [-1, 8 * 8 * 64]\n",
    "\n",
    "conv_2_flat = tf.reshape(conv_2_pool, shape = [-1, 8 * 8 * 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating the fully connected layers:\n",
    "\n",
    "full_layer_1 = tf.nn.relu(full_layer(conv_2_flat, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Dropout layer:\n",
    "\n",
    "full_layer_1_dropout = tf.nn.dropout(full_layer_1, keep_prob = hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output \n",
    "\n",
    "y_pred = full_layer(full_layer_1_dropout, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_true, logits = y_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Adam optimizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Session: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Sessions() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(5000):\n",
    "        batch = ch.next_batch(100)\n",
    "        sess.run(train, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "            print(sess.run(acc,feed_dict={x:ch.test_images,y_true:ch.test_labels,hold_prob:1.0}))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: -\n",
    "\n",
    "## We get an accuracy of 69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
